#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\begin_modules
customHeadersFooters
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize a4paper
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\rightmargin 3cm
\bottommargin 0cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle fancy
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
NN for Images Ex1 - HUJI
\end_layout

\begin_layout Author
Alon Emanuel
\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\addelta}{\text{Add}-\delta}
{\text{Add}-\delta}
\end_inset


\begin_inset FormulaMacro
\newcommand{\mle}{\text{MLE}}
{\text{MLE}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\count}{\text{COUNT}}
{\text{COUNT}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\comdots}{,\dots,}
{,\dots,}
\end_inset


\begin_inset FormulaMacro
\newcommand{\stop}{\text{STOP}}
{\text{STOP}}
\end_inset


\end_layout

\begin_layout Section
Programming Taskp
\end_layout

\begin_layout Subsection
Architecture Fine-Tuning
\end_layout

\begin_layout Itemize
To fine-tune our network architecture, we've ran the following procedure:
\end_layout

\begin_deeper
\begin_layout Itemize
We trained 10 different networks, each with a different number of filters.
\end_layout

\begin_layout Itemize
For each network, we saved the train and test errors, and plotted them as
 a function of the number of filters, to see which setting performed best.
\end_layout

\begin_layout Itemize
The change in the number of filters was done by changing the depth of the
 first convolutional layer (having it use less or more types of filters
 = having it output less or more channels).
\end_layout

\begin_layout Itemize
As can be seen in the plots, making the network too complex resulting in
 overfitting, while making it too simple resulted in underfitting.
\end_layout

\begin_layout Itemize
\begin_inset Note Note
status open

\begin_layout Plain Layout
ENTER PLOTS
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Subsection
Linear Model
\end_layout

\begin_layout Itemize
After removing the non-linear components, the performance of the net decreased
 drastically.
\end_layout

\begin_layout Itemize
The components include all the neuron activations used in this architecture.
 More specifically, the ReLU activation was removed, and the original neuron
 emissions remained the same.
\end_layout

\begin_layout Itemize
The results were as follows:
\end_layout

\begin_deeper
\begin_layout Itemize
Non-linear model (with ReLU activations):
\end_layout

\begin_deeper
\begin_layout Itemize
Train loss: 
\end_layout

\begin_layout Itemize
Test loss:
\end_layout

\end_deeper
\begin_layout Itemize
Linear model (without activation, and with 
\begin_inset Formula $120$
\end_inset

 neurons in the first FC layer):
\end_layout

\begin_deeper
\begin_layout Itemize
Train loss:
\end_layout

\begin_layout Itemize
Test loss:
\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Quotes eld
\end_inset

Deeper
\begin_inset Quotes erd
\end_inset

 linear model (without activation, and with 
\begin_inset Formula $240$
\end_inset

 neurons in the first FC layer):
\end_layout

\begin_deeper
\begin_layout Itemize
Train loss:
\end_layout

\begin_layout Itemize
Test loss:
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
The big decrease in performance of the linear model is due to its lack of
 expressiveness power.
\end_layout

\begin_layout Itemize
Since all layers in the linear model - convolutional, pooling and FC - are
 in fact linear operators, they can all be chained together to create one
 linear operator that'll perform the same operation.
\end_layout

\begin_layout Itemize
That is, the number and depth of the linear model doesn't have much effect
 after a certain point, since the expressive power can always be reduced
 to one single matrix multiplication.
\end_layout

\begin_layout Itemize
The non-linear components - the ReLU activations - allow the model to express
 man
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
why did the deep linear model perform worse than the shallow?
\end_layout

\begin_layout Plain Layout
understood in the test set due to overfitting, but how can this be possible
 in the train set?
\end_layout

\begin_layout Plain Layout
it has more power, how did it do that?
\end_layout

\end_inset


\end_layout

\begin_layout Standard
20 conv1 120 fc1 240 fc1
\end_layout

\begin_layout Enumerate
nonlin 
\end_layout

\begin_layout Enumerate
train 1.1355821559378505
\end_layout

\begin_layout Enumerate
test 1.194156339147687
\end_layout

\begin_layout Standard
lin
\end_layout

\begin_layout Standard
train 1.2240295984584093
\end_layout

\begin_layout Standard
test 1.2695638348668814
\end_layout

\begin_layout Standard
big lin
\end_layout

\begin_layout Standard
train 1.2715280068361758
\end_layout

\begin_layout Standard
test 1.3144673219695688
\end_layout

\begin_layout Standard
big linear
\end_layout

\begin_layout Standard
train loss: 1.23104951982975 test loss: 1.2680032145798206
\end_layout

\begin_layout Standard
linear
\end_layout

\begin_layout Standard
train loss: 1.2651844253891706 test loss: 1.3193395183444023
\end_layout

\begin_layout Subsection
Locality of the Receptive Field
\end_layout

\begin_layout Subsection
No Spatial Structure
\end_layout

\begin_layout Section
Theoretical Questions
\end_layout

\begin_layout Subsection*
Q1
\end_layout

\begin_layout Subsection*
Q2
\end_layout

\begin_layout Itemize
The order of the resulting 1D vector is unimportant.
\end_layout

\begin_layout Itemize
This is due to the fact that a FC takes into account all connections between
 the previous layer (the reshaped activation map) and the output layer.
 
\end_layout

\begin_layout Itemize
Thus, a neuron in the 2D activation map can be in any position, and the
 weights given to the edges going out of that neuron will change appropriately.
\end_layout

\begin_layout Subsection*
Q3
\end_layout

\end_body
\end_document
