#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\begin_modules
customHeadersFooters
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize a4paper
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\rightmargin 3cm
\bottommargin 0cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle fancy
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
NN for Images Ex1 - HUJI
\end_layout

\begin_layout Author
Alon Emanuel
\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\addelta}{\text{Add}-\delta}
{\text{Add}-\delta}
\end_inset


\begin_inset FormulaMacro
\newcommand{\mle}{\text{MLE}}
{\text{MLE}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\count}{\text{COUNT}}
{\text{COUNT}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\comdots}{,\dots,}
{,\dots,}
\end_inset


\begin_inset FormulaMacro
\newcommand{\stop}{\text{STOP}}
{\text{STOP}}
\end_inset


\end_layout

\begin_layout Section
Programming Taskp
\end_layout

\begin_layout Subsection
Architecture Fine-Tuning
\end_layout

\begin_layout Itemize
To fine-tune our network architecture, we've ran the following procedure:
\end_layout

\begin_deeper
\begin_layout Itemize
We trained 10 different networks, each with a different number of filters.
\end_layout

\begin_layout Itemize
For each network, we saved the train and test errors, and plotted them as
 a function of the number of filters, to see which setting performed best.
\end_layout

\begin_layout Itemize
The change in the number of filters was done by changing the depth of the
 first convolutional layer (having it use less or more types of filters
 = having it output less or more channels).
\end_layout

\begin_layout Itemize
As can be seen in the plots, making the network too complex resulting in
 overfitting, while making it too simple resulted in underfitting.
\end_layout

\begin_layout Itemize
\begin_inset Note Note
status open

\begin_layout Plain Layout
ENTER PLOTS
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Subsection
Linear Model
\end_layout

\begin_layout Itemize
After removing the non-linear components, the performance of the net decreased
 drastically.
\end_layout

\begin_layout Itemize
The components include all the neuron activations used in this architecture.
 More specifically, the ReLU activation was removed, and the original neuron
 emissions remained the same.
\end_layout

\begin_layout Itemize
The results were as follows:
\end_layout

\begin_deeper
\begin_layout Itemize
Non-linear model (with ReLU activations):
\end_layout

\begin_deeper
\begin_layout Itemize
Train loss: 1.1437
\end_layout

\begin_layout Itemize
Test loss: 1.2066
\end_layout

\end_deeper
\begin_layout Itemize
Linear model (without activation, and with 
\begin_inset Formula $120$
\end_inset

 neurons in the first FC layer):
\end_layout

\begin_deeper
\begin_layout Itemize
Train loss: 1.2391
\end_layout

\begin_layout Itemize
Test loss: 1.2825
\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Quotes eld
\end_inset

Deeper
\begin_inset Quotes erd
\end_inset

 linear model (without activation, and with 
\begin_inset Formula $240$
\end_inset

 neurons in the first FC layer):
\end_layout

\begin_deeper
\begin_layout Itemize
Train loss: 1.2119
\end_layout

\begin_layout Itemize
Test loss: 1.2571
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
The big decrease in performance of the linear model is due to its lack of
 expressiveness power.
\end_layout

\begin_layout Itemize
Since all layers in the linear model - convolutional, pooling and FC - are
 in fact linear operators, they can all be chained together to create one
 linear operator that'll perform the same operation.
\end_layout

\begin_layout Itemize
That is, the number and depth of the linear model doesn't have much effect
 after a certain point, since the expressive power can always be reduced
 to one single matrix multiplication.
\end_layout

\begin_layout Itemize
The non-linear components - the ReLU activations - allow the model to express
 a completely new set of functions.
\end_layout

\begin_layout Subsection
Locality of the Receptive Field
\end_layout

\begin_layout Itemize
Why larger filters don't help us test? maybe because it is still local?
\end_layout

\begin_layout Itemize
maybe because the net can train its weights to not focus on the edges? that
 is, small filters are a private case of big filters
\end_layout

\begin_layout Itemize
When training the network on images with reshuffled pixels, we get a dramatic
 decrease in performance.
 
\end_layout

\begin_layout Itemize
When trained with the same network parameters as in the previous section,
 we get the following results:
\end_layout

\begin_deeper
\begin_layout Itemize
Original images:
\end_layout

\begin_deeper
\begin_layout Itemize
Train loss: 1.0736
\end_layout

\begin_layout Itemize
Test loss: 1.13435
\end_layout

\begin_layout Itemize
Test accuracy: 60.14
\end_layout

\end_deeper
\begin_layout Itemize
Shuffled images:
\end_layout

\begin_deeper
\begin_layout Itemize
Train loss: 1.5758
\end_layout

\begin_layout Itemize
Test loss: 1.9407
\end_layout

\begin_layout Itemize
Test accuracy:
\end_layout

\begin_layout Itemize
train loss: 1.5758609551548959, train accuracy: 42.936 test loss: 1.613888527250289
9, test accuracy: 41.19
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
I think this is the case since shuffled images deem the convolutional filters
 unusable.
 Or in other words - the underlying premise for the convolutional layers
 to work - is the locality of the receptive field.
\end_layout

\begin_layout Itemize
Conv.
 filters give a 
\begin_inset Quotes eld
\end_inset

score
\begin_inset Quotes erd
\end_inset

 to local area of the image (e.g.
 a 5x5 crop), based on how much that area/crop fits the filter, or doesn't
 fit.
\end_layout

\begin_layout Itemize
For instance, some conv.
 filters check for edges in a given crop of an image.
 Once these pixels are shuffeled, there is no meaningful notion of 
\begin_inset Quotes eld
\end_inset

edge
\begin_inset Quotes erd
\end_inset

.
 
\end_layout

\begin_layout Itemize
Thus, the failure of the network is 
\end_layout

\begin_layout Subsection
No Spatial Structure
\end_layout

\begin_layout Section
Theoretical Questions
\end_layout

\begin_layout Subsection*
Q1
\end_layout

\begin_layout Subsection*
Q2
\end_layout

\begin_layout Itemize
The order of the resulting 1D vector is unimportant.
\end_layout

\begin_layout Itemize
This is due to the fact that a FC takes into account all connections between
 the previous layer (the reshaped activation map) and the output layer.
 
\end_layout

\begin_layout Itemize
Thus, a neuron in the 2D activation map can be in any position, and the
 weights given to the edges going out of that neuron will change appropriately.
\end_layout

\begin_layout Subsection*
Q3
\end_layout

\end_body
\end_document
